#!/usr/bin/env bash
# ============================================================================
# gpu v6.1 — Bridge Híbrida: Opus (cérebro) ↔ RTX 4090 (processador de dados)
# ============================================================================
#
# ARQUITETURA HÍBRIDA:
#   Opus DECIDE o que extrair → GPU EXTRAI dados (180 tok/s) → Opus ANALISA resultado
#
# MODELO ÚNICO — Qwen3-Coder-30B-MoE abliterated:
#   • 30B params totais, 128 experts, 3.3B ativados por token
#   • 178 tok/s — mais rápido que 7B dense (160 tok/s) com qualidade de 30B
#   • Zero censura (abliteration nos weights)
#   • 32K context window ativo (256K nativo, 32K = sweet spot VRAM)
#   • Override: GPU_MODEL=modelo gpu comando
#
# v6.1: GPU = processador puro (EXTRACT_RULE), temperaturas 4→3 tiers, all prompts rewritten
# v6.0: Auto-chunk, pipe, bulk paralelo, smart_extract head+mid+tail, classify JSON
# v5.0: 32K ctx, KEEP_ALIVE=-1, retry, temperatures, cache expansion
# Full changelog: docs/CHANGELOG.md
#
# Modos:
#   gpu ask "pergunta qualquer"              → resposta rápida
#   gpu scan <arquivo> "o que procurar"      → varre arquivo
#   gpu classify <arquivo>                   → classifica crash/log (JSON forçado)
#   gpu summarize <arquivo>                  → resume arquivo
#   gpu triage <bugreport.zip|log>           → pipeline completo Android
#   gpu search-vuln <arquivo> "contexto"     → auditoria segurança
#   gpu chunk <arquivo> "instrução" [linhas] → auto-fatia + paralelo + agrega
#   gpu bulk [-r] [-p] <pasta> "instrução"   → batch processing (-p = paralelo)
#   gpu pipe "instrução"                     → pipeline encadeado stdin→GPU→stdout
#   gpu diff <file1> <file2> "foco"          → compara arquivos
#   gpu stats                                → status completo do sistema
#
# Tudo roda na GPU local. Resultado vai para stdout (Opus lê e decide).
# ============================================================================

set -euo pipefail

# ─── Exit codes ─────────────────────────────────────────────────────────────────
EX_OK=0
EX_GENERIC=1            # erro genérico
EX_FILE=2               # arquivo não encontrado / binário / sem permissão
EX_OLLAMA=3             # ollama offline / modelo indisponível
EX_TIMEOUT=4            # timeout na query
EX_USAGE=5              # argumento faltando / uso incorreto

# ─── Config ─────────────────────────────────────────────────────────────────
NO_CACHE=false          # --no-cache flag: força reprocessamento
VERSION="6.1"
KALI_SAFE="/mnt/winraid/__KALI_SAFE"
export OLLAMA_MODELS="${KALI_SAFE}/models"
HOST="${OLLAMA_HOST:-http://127.0.0.1:11434}"
CURL_TIMEOUT="${GPU_TIMEOUT:-180}"      # timeout total
CURL_CONNECT_TIMEOUT=5                  # timeout de conexão
MAX_RETRIES=1                           # retry automático em falhas transitórias

# Modelo único: Qwen3-Coder-30B-MoE abliterated
# MoE (128 experts, 8 active) = velocidade de 3B + conhecimento de 30B
# Abliterated = censura removida nos weights (não só no prompt)
MODEL="${GPU_MODEL:-huihui_ai/qwen3-coder-abliterated:30b}"  # 178 tok/s, 20GB VRAM, zero censura

# Context window: 32K = sweet spot (93.5% VRAM, zero speed loss vs 8K)
# Modelo suporta 256K nativo mas 32K é o máximo que cabe na 4090 com margem
NUM_CTX=32768

# Cache de resultados no RAID
CACHE_DIR="${KALI_SAFE}/caches/gpu"
mkdir -p "$CACHE_DIR" 2>/dev/null || true

# Limites de conteúdo por comando (calibrados para 32K context window)
# Formula: (NUM_CTX - system_prompt_tokens - predict_tokens) * ~3.5 chars/token
CHARS_SMALL=8000        # classify, ask (sobra margem para system prompt longo)
CHARS_MEDIUM=48000      # scan (32K ctx = ~28K tokens disponíveis para input)
CHARS_LARGE=80000       # summarize (maximiza conteúdo processado)
CHARS_BULK=12000        # bulk (por arquivo, 2x mais que antes)
CHARS_VULN=64000        # search-vuln (3.2x mais contexto = extração profunda)
CHARS_DIFF=32000        # diff (cada arquivo = metade do limite)

# Tokens de resposta por comando
PREDICT_SMALL=512       # classify (JSON curto)
PREDICT_MEDIUM=1024     # ask, scan
PREDICT_LARGE=2048      # summarize
PREDICT_VULN=3072       # search-vuln (extraction output)
PREDICT_BULK=512        # bulk (por arquivo)

# Temperature por comando (otimizada por tipo de tarefa)
# GPU = processador puro → temperaturas baixas maximizam precisão de extração
TEMP_DETERMINISTIC=0.05 # classify, scan, vuln, diff, chunk — extração precisa
TEMP_BALANCED=0.2       # summarize, bulk — condensação com alguma liberdade
TEMP_CREATIVE=0.3       # ask — único comando onde GPU "pensa"

# Extraction rule: filosofia central da bridge
# GPU NÃO analisa, NÃO opina, NÃO raciocina. GPU EXTRAI e PROCESSA dados.
# Opus é quem analisa os resultados. GPU é grep inteligente a 180 tok/s.
EXTRACT_RULE="ROLE: You are a data processor, not an analyst. RULES: 1) ONLY extract information that exists literally in the input text. 2) NEVER invent values, line numbers, or details not present in the input. 3) NEVER add opinions, interpretations, or recommendations — that is the caller's job. 4) Output raw structured data. 5) If uncertain about a value, omit it rather than guess. 6) Quote the source text when referencing findings."

# ─── Cores (só stderr) ─────────────────────────────────────────────────────
if [[ -t 2 ]]; then
    C_RED='\033[0;31m'
    C_GREEN='\033[0;32m'
    C_YELLOW='\033[0;33m'
    C_BLUE='\033[0;34m'
    C_CYAN='\033[0;36m'
    C_BOLD='\033[1m'
    C_DIM='\033[2m'
    C_RESET='\033[0m'
else
    C_RED='' C_GREEN='' C_YELLOW='' C_BLUE='' C_CYAN='' C_BOLD='' C_DIM='' C_RESET=''
fi

# ─── Helpers ────────────────────────────────────────────────────────────────
log_info()  { echo -e "${C_BLUE}[gpu]${C_RESET} $*" >&2; }
log_ok()    { echo -e "${C_GREEN}[gpu]${C_RESET} $*" >&2; }
log_warn()  { echo -e "${C_YELLOW}[gpu]${C_RESET} $*" >&2; }
log_err()   { echo -e "${C_RED}[gpu]${C_RESET} $*" >&2; }
log_dim()   { echo -e "${C_DIM}[gpu] $*${C_RESET}" >&2; }

# Timer helpers
_timer_start() { _GPU_START=$(date +%s%N); }
_timer_end() {
    local elapsed=$(( ($(date +%s%N) - _GPU_START) / 1000000 ))
    if (( elapsed > 1000 )); then
        log_dim "⏱ $(( elapsed / 1000 )).$(( (elapsed % 1000) / 100 ))s"
    else
        log_dim "⏱ ${elapsed}ms"
    fi
}

ensure_ollama() {
    # Rápido: testa conexão
    if curl -sf --connect-timeout "$CURL_CONNECT_TIMEOUT" "${HOST}/api/tags" &>/dev/null; then
        validate_model
        return 0
    fi

    log_warn "Ollama offline. Tentando iniciar..."

    # Tenta systemctl (sem sudo se possível)
    if systemctl start ollama 2>/dev/null || sudo systemctl start ollama 2>/dev/null; then
        # Retry com backoff
        local attempt
        for attempt in 1 2 3 4 5; do
            sleep "$attempt"
            if curl -sf --connect-timeout "$CURL_CONNECT_TIMEOUT" "${HOST}/api/tags" &>/dev/null; then
                log_ok "Ollama iniciado (tentativa ${attempt})"
                validate_model
                return 0
            fi
        done
    fi

    log_err "FATAL: Não consegui iniciar Ollama em ${HOST}"
    exit $EX_OLLAMA
}

validate_model() {
    local available
    available=$(curl -sf --connect-timeout "$CURL_CONNECT_TIMEOUT" "${HOST}/api/tags" 2>/dev/null \
        | jq -r '.models[]?.name // empty' 2>/dev/null)

    if [[ -z "$available" ]]; then
        log_err "Nenhum modelo disponível no Ollama"
        exit $EX_OLLAMA
    fi

    if ! echo "$available" | grep -qF "$MODEL"; then
        log_warn "Modelo '${MODEL}' não encontrado. Disponíveis:"
        echo "$available" | while read -r m; do log_warn "  → $m"; done
        # Tenta usar o primeiro disponível
        MODEL=$(echo "$available" | head -1)
        log_warn "Usando fallback: ${MODEL}"
    fi
}

require_file() {
    local file="$1"
    if [[ ! -f "$file" ]]; then
        log_err "Arquivo não encontrado: $file"
        exit $EX_FILE
    fi
    if [[ ! -r "$file" ]]; then
        log_err "Sem permissão de leitura: $file"
        exit $EX_FILE
    fi
    if is_binary "$file"; then
        log_err "Arquivo binário não suportado: $file"
        exit $EX_FILE
    fi
}

require_dir() {
    local dir="$1"
    if [[ ! -d "$dir" ]]; then
        log_err "Diretório não encontrado: $dir"
        exit $EX_FILE
    fi
}

is_binary() {
    # Detecta arquivos binários (imagens, executáveis, etc)
    local file="$1"
    local mime
    mime=$(file --mime-encoding -b "$file" 2>/dev/null)
    [[ "$mime" == "binary" ]]
}

file_size_human() {
    local file="$1"
    local bytes
    bytes=$(stat -c%s "$file" 2>/dev/null || echo 0)
    if (( bytes > 1048576 )); then
        echo "$(( bytes / 1048576 ))MB"
    elif (( bytes > 1024 )); then
        echo "$(( bytes / 1024 ))KB"
    else
        echo "${bytes}B"
    fi
}

# Extração inteligente de conteúdo:
# - Arquivos pequenos: conteúdo completo
# - Arquivos grandes: head + middle + tail (3 seções para cobertura máxima)
smart_extract() {
    local file="$1"
    local max_chars="${2:-$CHARS_MEDIUM}"
    local total_lines
    local file_bytes

    total_lines=$(wc -l < "$file" 2>/dev/null || echo 0)
    file_bytes=$(stat -c%s "$file" 2>/dev/null || echo 0)

    if (( file_bytes <= max_chars )); then
        # Arquivo inteiro cabe
        cat "$file"
    else
        # Aviso de truncamento para o Opus saber que houve perda
        log_warn "Truncado: $(( file_bytes / 1024 ))KB → $(( max_chars / 1024 ))KB ($(( max_chars * 100 / file_bytes ))% do arquivo)"
        # Split: 40% head, 30% middle, 30% tail (não perde o core do arquivo)
        local head_chars=$(( max_chars * 40 / 100 ))
        local mid_chars=$(( max_chars * 30 / 100 ))
        local tail_chars=$(( max_chars * 30 / 100 ))

        # Head: primeiras linhas (imports, setup, declarations)
        head -c "$head_chars" "$file"
        echo ""
        echo ""
        echo "... [HEAD END — ${head_chars} chars] ..."
        echo ""

        # Middle: centro do arquivo (lógica core, funções principais)
        local mid_start_byte=$(( (file_bytes - mid_chars) / 2 ))
        echo "... [MIDDLE — starting at byte ${mid_start_byte} of ${file_bytes}] ..."
        echo ""
        dd if="$file" bs=4096 skip=$(( mid_start_byte / 4096 )) count=$(( (mid_chars + 4095) / 4096 )) 2>/dev/null | head -c "$mid_chars"
        echo ""
        echo ""
        echo "... [MIDDLE END] ..."
        echo ""

        # Tail: últimas linhas (exports, main, cleanup)
        echo "... [TAIL — last ${tail_chars} chars of ${total_lines} lines] ..."
        echo ""
        tail -c "$tail_chars" "$file"
    fi
}

# Cache key baseada em: arquivo + comando + modelo + versão do script
# VERSION no hash garante que mudança de prompts invalida cache antigo
cache_key() {
    local file="$1"
    local cmd="$2"
    local model="$3"
    local mtime
    mtime=$(stat -c%Y "$file" 2>/dev/null || echo 0)
    echo "${cmd}_$(echo "${file}_${mtime}_${model}_${VERSION}" | md5sum | cut -c1-16)"
}

cache_get() {
    local key="$1"
    # --no-cache: ignora cache
    $NO_CACHE && return 1
    local cache_file="${CACHE_DIR}/${key}.txt"
    if [[ -f "$cache_file" ]]; then
        # Cache válido por 24h
        local age=$(( $(date +%s) - $(stat -c%Y "$cache_file") ))
        if (( age < 86400 )); then
            log_dim "Cache hit: ${key} (age: ${age}s)"
            cat "$cache_file"
            return 0
        fi
    fi
    return 1
}

cache_put() {
    local key="$1"
    local content="$2"
    printf '%s\n' "$content" > "${CACHE_DIR}/${key}.txt" 2>/dev/null || true
}

# Core query function — fix SIGPIPE capturando em variável
# $1=system_prompt  $2=user_prompt  $3=num_predict  $4=temperature  $5=format(json|"")
ollama_query() {
    local system_prompt="$1"
    local user_prompt="$2"
    local num_predict="${3:-$PREDICT_MEDIUM}"
    local temperature="${4:-$TEMP_DETERMINISTIC}"
    local format="${5:-}"

    log_dim "Modelo: ${MODEL} | ctx:${NUM_CTX} temp:${temperature}$([ -n "$format" ] && echo " fmt:${format}")"

    local payload
    if [[ "$format" == "json" ]]; then
        payload=$(jq -n \
            --arg model "$MODEL" \
            --arg system "$system_prompt" \
            --arg prompt "$user_prompt" \
            --argjson predict "$num_predict" \
            --argjson temp "$temperature" \
            --argjson ctx "$NUM_CTX" \
            '{
                model: $model,
                system: $system,
                prompt: $prompt,
                stream: false,
                format: "json",
                options: {
                    temperature: $temp,
                    num_predict: $predict,
                    num_ctx: $ctx
                }
            }')
    else
        payload=$(jq -n \
            --arg model "$MODEL" \
            --arg system "$system_prompt" \
            --arg prompt "$user_prompt" \
            --argjson predict "$num_predict" \
            --argjson temp "$temperature" \
            --argjson ctx "$NUM_CTX" \
            '{
                model: $model,
                system: $system,
                prompt: $prompt,
                stream: false,
                options: {
                    temperature: $temp,
                    num_predict: $predict,
                    num_ctx: $ctx
                }
            }')
    fi

    # Retry loop — falhas transitórias (cold start, timeout)
    local attempt raw_response exit_code
    for attempt in $(seq 0 "$MAX_RETRIES"); do
        if (( attempt > 0 )); then
            log_warn "Retry ${attempt}/${MAX_RETRIES}..."
            sleep 2
        fi

        # Captura output em variável — NUNCA pipe direto (evita SIGPIPE)
        raw_response=$(curl -sf \
            --connect-timeout "$CURL_CONNECT_TIMEOUT" \
            --max-time "$CURL_TIMEOUT" \
            "${HOST}/api/generate" \
            -d "$payload" 2>/dev/null) && break

        exit_code=$?
        if (( exit_code == 28 )); then
            log_err "Timeout após ${CURL_TIMEOUT}s (tentativa $((attempt+1)))"
        else
            log_err "curl falhou (exit $exit_code, tentativa $((attempt+1)))"
        fi
    done

    if [[ -z "${raw_response:-}" ]]; then
        log_err "Todas as tentativas falharam. Ollama rodando?"
        return $EX_TIMEOUT
    fi

    # Extrai .response e métricas
    local response
    response=$(echo "$raw_response" | jq -r '.response // empty' 2>/dev/null)

    # Log de tokens usados (para o Opus acompanhar eficiência)
    local eval_count eval_duration prompt_eval_count
    eval_count=$(echo "$raw_response" | jq -r '.eval_count // 0' 2>/dev/null)
    eval_duration=$(echo "$raw_response" | jq -r '.eval_duration // 0' 2>/dev/null)
    prompt_eval_count=$(echo "$raw_response" | jq -r '.prompt_eval_count // 0' 2>/dev/null)
    if (( eval_duration > 0 )); then
        local tokens_per_sec=$(( eval_count * 1000000000 / eval_duration ))
        log_dim "Tokens: ${eval_count} (prompt:${prompt_eval_count}) | Speed: ${tokens_per_sec} tok/s"
    fi

    if [[ -z "$response" ]]; then
        # Fallback: tenta pegar erro
        local err
        err=$(echo "$raw_response" | jq -r '.error // empty' 2>/dev/null)
        if [[ -n "$err" ]]; then
            log_err "Ollama error: $err"
        else
            log_err "Resposta vazia do Ollama"
            log_dim "Raw: $(echo "$raw_response" | head -c 200)"
        fi
        return 1
    fi

    echo "$response"
}

# ─── Commands ───────────────────────────────────────────────────────────────

cmd_ask() {
    local question="$*"
    [[ -z "$question" ]] && { log_err "Uso: gpu ask \"pergunta\""; exit $EX_USAGE; }
    ensure_ollama

    _timer_start
    ollama_query \
        "You are a concise technical assistant. Answer directly, no fluff. Use structured formatting when helpful." \
        "$question" \
        "$PREDICT_MEDIUM" \
        "$TEMP_CREATIVE"
    _timer_end
}

cmd_scan() {
    local file="${1:-}"
    [[ -z "$file" ]] && { log_err "Uso: gpu scan <arquivo> \"o que procurar\""; exit $EX_USAGE; }
    shift
    local query="$*"
    [[ -z "$query" ]] && { log_err "Uso: gpu scan <arquivo> \"o que procurar\""; exit $EX_USAGE; }

    require_file "$file"
    ensure_ollama

    local size
    size=$(file_size_human "$file")
    local lines
    lines=$(wc -l < "$file")
    log_info "Scanning ${file} (${size}, ${lines}L) for: ${query}"

    # Auto-redirect para chunk se arquivo não cabe no contexto
    local file_bytes
    file_bytes=$(stat -c%s "$file" 2>/dev/null || echo 0)
    if (( file_bytes > CHARS_MEDIUM )); then
        log_warn "Arquivo excede contexto ($(( file_bytes / 1024 ))KB > $(( CHARS_MEDIUM / 1024 ))KB) — redirecionando para chunk"
        cmd_chunk "$file" "$query"
        return $?
    fi

    local content
    content=$(smart_extract "$file" "$CHARS_MEDIUM")

    # Cache: mesmo arquivo + mesma query = mesmo resultado
    local ckey
    ckey=$(cache_key "$file" "scan_$(echo "$query" | md5sum | cut -c1-8)" "$MODEL")
    if cache_get "$ckey"; then
        return 0
    fi

    _timer_start
    local result
    result=$(ollama_query \
        "${EXTRACT_RULE} Extract all occurrences matching the query. For each match return: the exact line content, line number if visible, and 1-2 lines of surrounding context. Output as a structured list. Do not interpret or judge the findings." \
        "Extract from this file all occurrences of: ${query}

File: $(basename "$file") | Size: ${size} | Lines: ${lines}

--- FILE CONTENT ---
${content}
--- END ---" \
        "$PREDICT_MEDIUM" \
        "$TEMP_DETERMINISTIC")
    echo "$result"
    [[ -n "$result" ]] && cache_put "$ckey" "$result"
    _timer_end
}

cmd_classify() {
    local file="${1:-}"
    [[ -z "$file" ]] && { log_err "Uso: gpu classify <arquivo>"; exit $EX_USAGE; }

    require_file "$file"
    ensure_ollama

    log_info "Classifying $(basename "$file") ($(file_size_human "$file"))"

    local content
    content=$(smart_extract "$file" "$CHARS_SMALL")

    # Cache: classify muda pouco
    local ckey
    ckey=$(cache_key "$file" "classify" "$MODEL")
    if cache_get "$ckey"; then
        return 0
    fi

    _timer_start
    local result
    result=$(ollama_query \
        'You classify crash/log/error snippets. Return valid JSON with this exact structure: {"type": "crash|error|warning|info|mixed", "severity": "critical|high|medium|low", "subsystem": "component name", "root_cause": "1 line", "summary": "1 line description", "actionable": true/false}' \
        "Classify this file:

${content}" \
        "$PREDICT_SMALL" \
        "$TEMP_DETERMINISTIC" \
        "json")
    echo "$result"
    [[ -n "$result" ]] && cache_put "$ckey" "$result"
    _timer_end
}

cmd_summarize() {
    local file="${1:-}"
    [[ -z "$file" ]] && { log_err "Uso: gpu summarize <arquivo>"; exit $EX_USAGE; }

    require_file "$file"
    ensure_ollama

    local size
    size=$(file_size_human "$file")
    local lines
    lines=$(wc -l < "$file")
    log_info "Summarizing $(basename "$file") (${size}, ${lines}L)"

    # Auto-redirect para chunk se arquivo não cabe no contexto
    local file_bytes
    file_bytes=$(stat -c%s "$file" 2>/dev/null || echo 0)
    if (( file_bytes > CHARS_LARGE )); then
        log_warn "Arquivo excede contexto ($(( file_bytes / 1024 ))KB > $(( CHARS_LARGE / 1024 ))KB) — redirecionando para chunk"
        cmd_chunk "$file" "condense into structured outline, extract main sections and key data points"
        return $?
    fi

    local content
    content=$(smart_extract "$file" "$CHARS_LARGE")

    # Cache: evita reprocessar o mesmo arquivo
    local ckey
    ckey=$(cache_key "$file" "summarize" "$MODEL")
    if cache_get "$ckey"; then
        return 0
    fi

    _timer_start
    local result
    result=$(ollama_query \
        "${EXTRACT_RULE} Condense this document into a structured outline. Extract: main sections/topics, key data points (numbers, names, values), and any items marked as important/critical/TODO/FIXME. Output as a hierarchical list. Do not add interpretation." \
        "Condense this document (${lines} lines, ${size}) into a structured outline:

${content}" \
        "$PREDICT_LARGE" \
        "$TEMP_BALANCED")
    echo "$result"
    [[ -n "$result" ]] && cache_put "$ckey" "$result"
    _timer_end
}

cmd_triage() {
    local input="${1:-}"
    [[ -z "$input" ]] && { log_err "Uso: gpu triage <bugreport.zip|logfile>"; exit $EX_USAGE; }

    if [[ ! -f "$input" ]]; then
        log_err "Arquivo não encontrado: $input"
        exit $EX_FILE
    fi

    local triage_script="${KALI_SAFE}/scripts/triage.py"
    if [[ ! -f "$triage_script" ]]; then
        log_err "triage.py não encontrado em: $triage_script"
        log_warn "Alternativa: use 'gpu classify' ou 'gpu summarize' para processamento direto"
        exit $EX_FILE
    fi

    log_info "Running full triage pipeline on $(basename "$input")..."
    _timer_start

    if [[ "$input" == *.zip ]]; then
        python3 "$triage_script" --bugreport "$input"
    else
        python3 "$triage_script" --logcat "$input"
    fi
    _timer_end
}

cmd_search_vuln() {
    local file="${1:-}"
    [[ -z "$file" ]] && { log_err "Uso: gpu search-vuln <arquivo> \"padrões extras\""; exit $EX_USAGE; }
    shift
    local extra_patterns="${*:-}"

    require_file "$file"

    local size
    size=$(file_size_human "$file")
    local total_lines
    total_lines=$(wc -l < "$file")
    log_info "search-vuln v3: $(basename "$file") (${size}, ${total_lines}L)"

    # ─── FASE 1: PRE-SCAN LOCAL (grep -nE, zero GPU) ───────────────────────
    # 36 regex concretos — só padrões que EXISTEM no código
    local -a VULN_PATTERNS=(
        # JS/Node — injection & exec
        'eval\s*\('
        '\bexec\s*\('
        'execSync\s*\('
        'child_process'
        'spawn\s*\('
        'Function\s*\('
        'setTimeout\s*\('
        'setInterval\s*\('
        # DOM — XSS vectors
        'innerHTML'
        'outerHTML'
        'document\.write'
        'insertAdjacentHTML'
        # SQL — concatenation
        '\.query\s*\('
        '\.execute\s*\('
        'SELECT\s.*\+|INSERT\s.*\+'
        # Hardcoded secrets
        'password\s*=\s*["\x27]'
        'api[_-]?key\s*=\s*["\x27]'
        'secret\s*=\s*["\x27]'
        'token\s*=\s*["\x27]'
        # TLS bypass
        'rejectUnauthorized\s*:\s*false'
        'NODE_TLS_REJECT_UNAUTHORIZED'
        # File ops with user input
        'readFile.*req\.|writeFile.*req\.'
        # Deserialization
        'unserialize\s*\('
        'pickle\.loads\s*\('
        'yaml\.load\s*\('
        # Java/Android — exec & reflection
        'Runtime\.getRuntime\(\)\.exec'
        'ProcessBuilder'
        'ScriptEngine.*eval'
        'Class\.forName'
        'ObjectInputStream'
        'Cipher\.getInstance\s*\(\s*["\x27]'
        # Android — WebView
        'addJavascriptInterface'
        'setJavaScriptEnabled\s*\(\s*true'
        'loadUrl.*javascript:'
        'WebView.*loadData'
        # Android — content provider
        'ContentResolver.*query'
        'rawQuery\s*\('
        'execSQL\s*\('
    )

    # Monta regex combinado
    local combined_regex
    combined_regex=$(printf '%s|' "${VULN_PATTERNS[@]}")
    combined_regex="${combined_regex%|}"  # remove trailing |

    # Adiciona padrões extras do usuário
    if [[ -n "$extra_patterns" ]]; then
        local extra
        for extra in $extra_patterns; do
            combined_regex+="|${extra}"
        done
        log_info "Padrões extras: ${extra_patterns}"
    fi

    # Pre-scan com grep local — ZERO GPU
    local grep_hits
    grep_hits=$(grep -nE "$combined_regex" "$file" 2>/dev/null || true)

    if [[ -z "$grep_hits" ]]; then
        log_info "PRE-SCAN: 0 hits — arquivo limpo, GPU não chamada"
        echo "NO PATTERNS FOUND"
        return 0
    fi

    local hit_count
    hit_count=$(echo "$grep_hits" | wc -l)
    log_info "PRE-SCAN: ${hit_count} hits encontrados — enviando para GPU validar"

    # ─── FASE 2: GPU COMO VALIDADOR ────────────────────────────────────────
    # GPU recebe APENAS os hits do grep, não o arquivo inteiro
    # Pergunta: "este hit é perigoso ou falso positivo?"

    ensure_ollama

    # Cache
    local ckey
    ckey=$(cache_key "$file" "vuln3_$(echo "$extra_patterns" | md5sum | cut -c1-8)" "$MODEL")
    if cache_get "$ckey"; then
        return 0
    fi

    _timer_start
    local result
    result=$(ollama_query \
        "${EXTRACT_RULE} You are a pattern validator. You receive grep hits from a security scan.
For each hit, determine if it is a REAL security-relevant pattern or a FALSE POSITIVE.

FALSE POSITIVES to reject:
- Pattern inside a comment (// or /* */ or # or --)
- Pattern inside a string literal used as a label/message (not executed)
- Safe usage: parameterized queries (\$1, ?, :param), env var lookups (process.env), SafeLoader
- Test/mock/example code that is clearly not production
- Import/require statements (just loading a module is not a vulnerability)

For each CONFIRMED hit, output exactly:
LINE: <number>
CODE: <exact code from the hit>
PATTERN: <which pattern matched>

For rejected hits, output nothing (skip silently).
If ALL hits are false positives, output: ALL HITS REJECTED — NO REAL PATTERNS

Do NOT add severity, impact, recommendations, or opinions." \
        "File: $(basename "$file")
Language: $(file -b "$file" 2>/dev/null || echo unknown)

--- GREP HITS ---
${grep_hits}
--- END HITS ---" \
        "$PREDICT_VULN" \
        "$TEMP_DETERMINISTIC")

    # ─── FASE 3: AUTO-VALIDAÇÃO TRIPLA ─────────────────────────────────────
    # Check 1: line number exists in file
    # Check 2: sed extracts real content
    # Check 3: real line matches at least 1 of the 36 patterns

    local confirmed="" rejected_count=0 confirmed_count=0

    if [[ -n "$result" ]] && ! echo "$result" | grep -q "ALL HITS REJECTED\|NO REAL PATTERNS\|NO PATTERNS FOUND"; then
        while IFS= read -r line; do
            if [[ "$line" =~ ^LINE:\ *([0-9]+) ]]; then
                local lnum="${BASH_REMATCH[1]}"

                # Read CODE and PATTERN lines
                local code_line pattern_line
                IFS= read -r code_line 2>/dev/null || true
                IFS= read -r pattern_line 2>/dev/null || true

                # Check 1: line exists
                if (( lnum < 1 || lnum > total_lines )); then
                    rejected_count=$((rejected_count + 1))
                    continue
                fi

                # Check 2: extract real line
                local real_line
                real_line=$(sed -n "${lnum}p" "$file" 2>/dev/null)
                if [[ -z "$real_line" ]]; then
                    rejected_count=$((rejected_count + 1))
                    continue
                fi

                # Check 3: real line matches at least 1 concrete pattern
                if ! echo "$real_line" | grep -qE "$combined_regex" 2>/dev/null; then
                    rejected_count=$((rejected_count + 1))
                    continue
                fi

                # All 3 checks passed — confirmed
                confirmed_count=$((confirmed_count + 1))
                confirmed+="LINE: ${lnum}
CODE: ${real_line}
PATTERN: ${pattern_line#PATTERN: }

"
            fi
        done <<< "$result"
    fi

    # ─── OUTPUT ─────────────────────────────────────────────────────────────
    echo "## search-vuln v3: $(basename "$file")"
    echo ""
    echo "**Pre-scan**: ${hit_count} grep hits"
    echo "**Confirmed**: ${confirmed_count} patterns"
    echo "**Rejected**: ${rejected_count} (auto-validation)"
    echo ""

    if (( confirmed_count > 0 )); then
        echo "### Confirmed Patterns"
        echo ""
        echo "$confirmed"
    else
        echo "All hits were false positives or rejected by auto-validation."
    fi

    # Cache o resultado final
    [[ -n "$confirmed" ]] && cache_put "$ckey" "$confirmed"
    _timer_end
}

cmd_bulk() {
    local recursive=false
    local parallel=false
    # Parse flags
    while [[ "${1:-}" == -* ]]; do
        case "$1" in
            -r) recursive=true; shift ;;
            -p) parallel=true; shift ;;
            *) break ;;
        esac
    done

    local dir="${1:-}"
    [[ -z "$dir" ]] && { log_err "Uso: gpu bulk [-r] [-p] <dir> \"instrução\""; exit $EX_USAGE; }
    shift
    local instruction="$*"
    [[ -z "$instruction" ]] && { log_err "Uso: gpu bulk [-r] [-p] <dir> \"instrução\""; exit $EX_USAGE; }

    require_dir "$dir"
    ensure_ollama

    _timer_start

    echo "## Bulk Processing: $(basename "$dir")"
    echo "**Instruction**: ${instruction}"
    echo ""

    local count=0
    local skipped=0
    local files=()

    # Coleta arquivos (recursivo ou não)
    if $recursive; then
        while IFS= read -r -d '' f; do
            files+=("$f")
        done < <(find "$dir" -type f -print0 2>/dev/null | sort -z)
    else
        for f in "$dir"/*; do
            [[ -f "$f" ]] && files+=("$f")
        done
    fi

    # Filtra binários e vazios antecipadamente
    local valid_files=()
    for f in "${files[@]}"; do
        if is_binary "$f"; then
            ((skipped++))
            log_dim "Skip (binary): $(basename "$f")"
        elif [[ ! -s "$f" ]]; then
            ((skipped++))
            log_dim "Skip (empty): $(basename "$f")"
        else
            valid_files+=("$f")
        fi
    done

    local total=${#valid_files[@]}
    log_info "Found ${total} processable files in $(basename "$dir") $(${recursive} && echo '(recursive)' || echo '')"

    if $parallel && (( total > 1 )); then
        # ── Parallel mode: 2 requests simultâneos (usa OLLAMA_NUM_PARALLEL=2) ──
        log_info "⚡ Parallel mode: 2 workers"
        local tmpdir
        tmpdir=$(mktemp -d)
        trap "rm -rf '$tmpdir'" EXIT

        local running=0
        local idx=0

        for f in "${valid_files[@]}"; do
            ((idx++))
            local relpath="${f#$dir/}"
            local outfile="${tmpdir}/result_$(printf '%04d' $idx).md"
            local content
            content=$(smart_extract "$f" "$CHARS_BULK")

            log_info "[${idx}/${total}] Dispatching: ${relpath}"

            # Lança em background (stderr redireciona pra log de erro)
            (
                local r err_file="${tmpdir}/err_$(printf '%04d' $idx).log"
                r=$(ollama_query \
                    "${EXTRACT_RULE} Process the file according to the instruction. Output structured data only." \
                    "Instruction: ${instruction}

--- FILE: ${relpath} ($(file_size_human "$f")) ---
${content}
--- END ---" \
                    "$PREDICT_BULK" \
                    "$TEMP_BALANCED" 2>"$err_file")
                if [[ -z "$r" ]]; then
                    r="⚠️ GPU processing failed for this file"
                fi
                {
                    echo "### ${relpath}"
                    echo ""
                    echo "$r"
                    echo ""
                    echo "---"
                    echo ""
                } > "$outfile"
            ) &

            ((running++))

            # Espera quando atingir 2 paralelos
            if (( running >= 2 )); then
                wait -n 2>/dev/null || true
                ((running--))
            fi
        done

        # Espera todos terminarem
        wait

        # Output ordenado
        for outfile in "$tmpdir"/result_*.md; do
            [[ -f "$outfile" ]] && cat "$outfile"
        done
        count=$total
    else
        # ── Sequential mode (default) ──
        for f in "${valid_files[@]}"; do
            ((count++))
            local relpath="${f#$dir/}"
            log_info "[${count}/${total}] Processing: ${relpath}"

            echo "### ${relpath}"
            echo ""

            local content
            content=$(smart_extract "$f" "$CHARS_BULK")

            ollama_query \
                "${EXTRACT_RULE} Process the file according to the instruction. Output structured data only." \
                "Instruction: ${instruction}

--- FILE: ${relpath} ($(file_size_human "$f")) ---
${content}
--- END ---" \
                "$PREDICT_BULK" \
                "$TEMP_BALANCED"

            echo ""
            echo "---"
            echo ""
        done
    fi

    echo ""
    echo "**Total**: ${count} processed, ${skipped} skipped"
    log_ok "Bulk complete: ${count} processed, ${skipped} skipped"
    _timer_end
}

cmd_chunk() {
    # Auto-chunk: fatia arquivo grande, processa cada chunk na GPU em paralelo, agrega
    local file="${1:-}"
    [[ -z "$file" ]] && { log_err "Uso: gpu chunk <arquivo> \"instrução\" [chunk_lines] [overlap_lines]"; exit $EX_USAGE; }
    shift
    local instruction="${1:-extract bugs, security issues, and code smells}"
    shift 2>/dev/null || true
    local chunk_lines="${1:-800}"
    shift 2>/dev/null || true
    local overlap_lines="${1:-50}"

    require_file "$file"
    ensure_ollama

    local size total_lines
    size=$(file_size_human "$file")
    total_lines=$(wc -l < "$file")

    log_info "Chunk processing: $(basename "$file") (${size}, ${total_lines}L)"
    log_info "Strategy: ${chunk_lines}L chunks, ${overlap_lines}L overlap, parallel=2"

    # Se cabe inteiro → manda direto sem chunking
    if (( $(stat -c%s "$file") <= CHARS_MEDIUM )); then
        log_info "File fits in context window — processing directly"
        cmd_scan "$file" "$instruction"
        return $?
    fi

    _timer_start

    # Cria chunks com overlap
    local tmpdir
    tmpdir=$(mktemp -d)
    trap "rm -rf '$tmpdir'" EXIT

    local chunk_num=0
    local start_line=1

    while (( start_line <= total_lines )); do
        ((chunk_num++))
        local end_line=$(( start_line + chunk_lines - 1 ))
        (( end_line > total_lines )) && end_line=$total_lines

        local chunk_file="${tmpdir}/chunk_$(printf '%03d' $chunk_num).txt"
        sed -n "${start_line},${end_line}p" "$file" > "$chunk_file"

        # Prepend header com contexto de posição
        local header="[CHUNK ${chunk_num} | Lines ${start_line}-${end_line} of ${total_lines} | File: $(basename "$file")]"
        local tmp_with_header="${tmpdir}/h_$(printf '%03d' $chunk_num).txt"
        {
            echo "$header"
            echo ""
            cat "$chunk_file"
        } > "$tmp_with_header"
        mv "$tmp_with_header" "$chunk_file"

        # Salva range real para uso na agregação
        echo "${start_line} ${end_line}" > "${tmpdir}/range_$(printf '%03d' $chunk_num).txt"

        # Se chegou ao fim do arquivo, para
        (( end_line >= total_lines )) && break

        # Próximo chunk começa overlap_lines antes do fim (para contexto)
        start_line=$(( end_line - overlap_lines + 1 ))
    done

    local num_chunks=$chunk_num
    log_info "Split into ${num_chunks} chunks, processing with 2 parallel workers..."

    echo "## Chunk Processing: $(basename "$file")"
    echo "**File**: ${size}, ${total_lines} lines"
    echo "**Chunks**: ${num_chunks} (${chunk_lines}L each, ${overlap_lines}L overlap)"
    echo "**Instruction**: ${instruction}"
    echo ""

    # Processa chunks em paralelo (2 workers)
    local running=0
    for i in $(seq 1 "$num_chunks"); do
        local chunk_file="${tmpdir}/chunk_$(printf '%03d' $i).txt"
        local result_file="${tmpdir}/result_$(printf '%03d' $i).md"

        log_info "[${i}/${num_chunks}] Dispatching chunk"

        (
            local content err_file="${tmpdir}/err_$(printf '%03d' $i).log"
            content=$(cat "$chunk_file")
            local r
            r=$(ollama_query \
                "${EXTRACT_RULE} You are processing a CHUNK of a larger file. The chunk header shows the line range. Extract items matching the instruction from THIS chunk only. Quote exact content. Use line numbers from the header. Output as a structured list." \
                "Instruction: ${instruction}

--- CHUNK CONTENT ---
${content}
--- END CHUNK ---" \
                "$PREDICT_MEDIUM" \
                "$TEMP_DETERMINISTIC" 2>"$err_file")
            if [[ -z "$r" ]]; then
                r="⚠️ GPU processing failed for this chunk"
            fi
            echo "$r" > "$result_file"
        ) &

        ((running++))
        if (( running >= 2 )); then
            wait -n 2>/dev/null || true
            ((running--))
        fi
    done
    wait

    # Agrega resultados ordenados
    for i in $(seq 1 "$num_chunks"); do
        local result_file="${tmpdir}/result_$(printf '%03d' $i).md"
        if [[ -f "$result_file" && -s "$result_file" ]]; then
            local range_file="${tmpdir}/range_$(printf '%03d' $i).txt"
            local start_l end_l
            if [[ -f "$range_file" ]]; then
                read -r start_l end_l < "$range_file"
            else
                start_l="?"
                end_l="?"
            fi
            echo "### Chunk ${i} (Lines ${start_l}–${end_l})"
            echo ""
            cat "$result_file"
            echo ""
            echo "---"
            echo ""
        fi
    done

    echo ""
    echo "**Summary**: ${num_chunks} chunks processed from ${total_lines} lines"
    echo "**Note**: Opus should now cross-reference and deduplicate findings across chunks"
    log_ok "Chunk processing complete: ${num_chunks} chunks"
    _timer_end
}

cmd_diff() {
    local file1="${1:-}"
    local file2="${2:-}"
    [[ -z "$file1" || -z "$file2" ]] && { log_err "Uso: gpu diff <file1> <file2> [\"foco\"]"; exit $EX_USAGE; }
    shift 2
    local focus="${*:-general comparison}"

    require_file "$file1"
    require_file "$file2"
    ensure_ollama

    log_info "Comparing: $(basename "$file1") vs $(basename "$file2")"

    local content1 content2
    content1=$(smart_extract "$file1" "$CHARS_DIFF")
    content2=$(smart_extract "$file2" "$CHARS_DIFF")

    # Cache: mesmos 2 ficheiros = mesmo diff
    local ckey
    ckey=$(cache_key "$file1" "diff_$(echo "${file2}_${focus}" | md5sum | cut -c1-8)" "$MODEL")
    if cache_get "$ckey"; then
        return 0
    fi

    _timer_start
    local result
    result=$(ollama_query \
        "${EXTRACT_RULE} Extract the structural differences between these two files. For each difference output: what changed (added/removed/modified), the exact content from each file, and the location. Do not judge whether changes are good or bad." \
        "Compare these files. Focus: ${focus}

--- FILE 1: $(basename "$file1") ($(file_size_human "$file1")) ---
${content1}
--- END FILE 1 ---

--- FILE 2: $(basename "$file2") ($(file_size_human "$file2")) ---
${content2}
--- END FILE 2 ---" \
        "$PREDICT_LARGE" \
        "$TEMP_DETERMINISTIC")
    echo "$result"
    [[ -n "$result" ]] && cache_put "$ckey" "$result"
    _timer_end
}

cmd_pipe() {
    # Pipeline encadeado: stdin do pipe anterior → processamento GPU → stdout
    # Uso: gpu scan file "bugs" | gpu pipe "priorize por severidade" | gpu pipe "sugira fixes"
    # Uso: cat findings.txt | gpu pipe "classifique cada finding"
    local instruction="$*"
    [[ -z "$instruction" ]] && { log_err "Uso: echo 'data' | gpu pipe \"instrução\""; exit $EX_USAGE; }

    # Lê stdin (output do comando anterior)
    local stdin_data
    if [[ -t 0 ]]; then
        log_err "gpu pipe espera input via stdin (pipe)"
        log_err "Exemplo: gpu scan file.js \"bugs\" | gpu pipe \"priorize\""
        exit $EX_USAGE
    fi
    stdin_data=$(cat)

    if [[ -z "$stdin_data" ]]; then
        log_err "stdin vazio — nada para processar"
        exit $EX_USAGE
    fi

    local input_size=${#stdin_data}
    # Trunca se excede contexto
    if (( input_size > CHARS_MEDIUM )); then
        log_warn "Input truncado: ${input_size} → ${CHARS_MEDIUM} chars"
        stdin_data="${stdin_data:0:$CHARS_MEDIUM}"
    fi

    ensure_ollama

    log_info "Pipe: ${instruction:0:60}... (${input_size} chars input)"

    _timer_start
    ollama_query \
        "${EXTRACT_RULE} You are a data transformation step in a pipeline. Apply the instruction to transform the input data. Output clean, structured data ready for the next step. Do not add commentary — only output the transformed result." \
        "Instruction: ${instruction}

--- INPUT FROM PREVIOUS STEP ---
${stdin_data}
--- END INPUT ---" \
        "$PREDICT_LARGE" \
        "$TEMP_DETERMINISTIC"
    _timer_end
}

cmd_cache_clean() {
    local cleaned=0
    local kept=0
    local now
    now=$(date +%s)

    if [[ ! -d "$CACHE_DIR" ]]; then
        log_info "Cache dir não existe: $CACHE_DIR"
        return 0
    fi

    local total
    total=$(find "$CACHE_DIR" -name '*.txt' 2>/dev/null | wc -l)

    while IFS= read -r -d '' cache_file; do
        local age=$(( now - $(stat -c%Y "$cache_file") ))
        if (( age > 86400 )); then
            rm -f "$cache_file"
            ((cleaned++))
        else
            ((kept++))
        fi
    done < <(find "$CACHE_DIR" -name '*.txt' -print0 2>/dev/null)

    local freed_size
    if (( cleaned > 0 )); then
        freed_size="(cache dir: $(du -sh "$CACHE_DIR" 2>/dev/null | cut -f1))"
    else
        freed_size=""
    fi

    log_ok "Cache cleanup: ${cleaned} expired removed, ${kept} valid kept ${freed_size}"
}

cmd_stats() {
    echo "## gpu Bridge v${VERSION} — System Stats"
    echo ""

    # Ollama status
    if curl -sf --connect-timeout 2 "${HOST}/api/tags" &>/dev/null; then
        echo "**Ollama**: ✅ Online (${HOST})"
        local models
        models=$(curl -sf "${HOST}/api/tags" | jq -r '.models[]? | "  - \(.name) (\(.details.parameter_size // "?") \(.details.quantization_level // ""))"' 2>/dev/null)
        echo "**Models**:"
        echo "$models"
    else
        echo "**Ollama**: ❌ Offline"
    fi
    echo ""

    # GPU status
    if command -v nvidia-smi &>/dev/null; then
        echo "**GPU**:"
        nvidia-smi --query-gpu=name,temperature.gpu,utilization.gpu,memory.used,memory.total \
            --format=csv,noheader,nounits 2>/dev/null | \
            awk -F', ' '{printf "  - %s | Temp: %s°C | Load: %s%% | VRAM: %s/%sMB\n", $1, $2, $3, $4, $5}'
    fi
    echo ""

    # Model config (dinâmico via /api/ps)
    echo "**Model**:"
    echo "  - Active: ${MODEL}"
    local model_info
    model_info=$(curl -sf --connect-timeout 2 "${HOST}/api/ps" 2>/dev/null)
    if [[ -n "$model_info" ]]; then
        local vram_mb family ctx_loaded
        vram_mb=$(echo "$model_info" | jq -r '.models[0].size_vram // 0' 2>/dev/null)
        vram_mb=$(( vram_mb / 1048576 ))
        family=$(echo "$model_info" | jq -r '.models[0].details.family // "unknown"' 2>/dev/null)
        ctx_loaded=$(echo "$model_info" | jq -r '.models[0].context_length // 0' 2>/dev/null)
        echo "  - Family: ${family} | Context loaded: ${ctx_loaded} | VRAM: ${vram_mb}MB"
    else
        echo "  - (modelo não carregado)"
    fi
    echo ""

    # Cache stats
    local cache_count cache_size
    cache_count=$(find "$CACHE_DIR" -name '*.txt' 2>/dev/null | wc -l)
    cache_size=$(du -sh "$CACHE_DIR" 2>/dev/null | cut -f1)
    echo "**Cache**:"
    echo "  - Location: ${CACHE_DIR}"
    echo "  - Entries: ${cache_count} (${cache_size:-0})"
    echo ""

    # RAID storage
    echo "**RAID Storage**:"
    df -h /mnt/winraid/ 2>/dev/null | tail -1 | awk '{printf "  - Total: %s | Used: %s (%s) | Free: %s\n", $2, $3, $5, $4}'
    echo ""

    # Config
    echo "**Config**:"
    echo "  - Version: ${VERSION}"
    echo "  - Timeout: ${CURL_TIMEOUT}s"
    echo "  - Content limits: small=${CHARS_SMALL} med=${CHARS_MEDIUM} large=${CHARS_LARGE} vuln=${CHARS_VULN} diff=${CHARS_DIFF}"
    echo "  - Temperatures: extract=${TEMP_DETERMINISTIC} condense=${TEMP_BALANCED} creative=${TEMP_CREATIVE}"
    echo "  - Retry: ${MAX_RETRIES}x"
    echo "  - OLLAMA_MODELS: ${OLLAMA_MODELS}"
    echo ""

    # Ollama optimizations (from systemd override)
    echo "**Optimizations**:"
    local ollama_pid
    ollama_pid=$(pgrep -x ollama 2>/dev/null | head -1)
    if [[ -n "$ollama_pid" ]]; then
        local env_data
        env_data=$(sudo cat /proc/${ollama_pid}/environ 2>/dev/null | tr '\0' '\n')
        local fa kv ka np tmpdir
        fa=$(echo "$env_data" | command grep -oP 'OLLAMA_FLASH_ATTENTION=\K.*' 2>/dev/null || echo "?")
        kv=$(echo "$env_data" | command grep -oP 'OLLAMA_KV_CACHE_TYPE=\K.*' 2>/dev/null || echo "?")
        ka=$(echo "$env_data" | command grep -oP 'OLLAMA_KEEP_ALIVE=\K.*' 2>/dev/null || echo "?")
        np=$(echo "$env_data" | command grep -oP 'OLLAMA_NUM_PARALLEL=\K.*' 2>/dev/null || echo "?")
        tmpdir=$(echo "$env_data" | command grep -oP 'OLLAMA_TMPDIR=\K.*' 2>/dev/null || echo "?")
        echo "  - Flash Attention: ${fa}"
        echo "  - KV Cache Type: ${kv}"
        echo "  - Keep Alive: ${ka}"
        echo "  - Parallel Requests: ${np}"
        echo "  - TMPDIR: ${tmpdir}"
    else
        echo "  - (Ollama não está rodando)"
    fi
}

cmd_bench() {
    ensure_ollama
    log_info "Benchmark: disparando query curta para medir tok/s..."
    _timer_start
    local raw
    raw=$(curl -sf --connect-timeout "$CURL_CONNECT_TIMEOUT" --max-time 30 \
        "${HOST}/api/generate" \
        -d "$(jq -n --arg model "$MODEL" --argjson ctx "$NUM_CTX" '{
            model: $model,
            system: "Respond with exactly 100 words about binary exploitation.",
            prompt: "Go.",
            stream: false,
            options: { temperature: 0.3, num_predict: 200, num_ctx: $ctx }
        }')" 2>/dev/null)

    if [[ -z "$raw" ]]; then
        log_err "Benchmark falhou — Ollama não respondeu"
        return $EX_TIMEOUT
    fi

    local eval_count eval_duration prompt_eval_count prompt_duration
    eval_count=$(echo "$raw" | jq -r '.eval_count // 0')
    eval_duration=$(echo "$raw" | jq -r '.eval_duration // 0')
    prompt_eval_count=$(echo "$raw" | jq -r '.prompt_eval_count // 0')
    prompt_duration=$(echo "$raw" | jq -r '.prompt_eval_duration // 0')

    echo "## Benchmark Results"
    echo "**Model**: ${MODEL}"
    if (( eval_duration > 0 )); then
        local gen_tps=$(( eval_count * 1000000000 / eval_duration ))
        echo "**Generation**: ${eval_count} tokens @ **${gen_tps} tok/s**"
    fi
    if (( prompt_duration > 0 )); then
        local prompt_tps=$(( prompt_eval_count * 1000000000 / prompt_duration ))
        echo "**Prompt eval**: ${prompt_eval_count} tokens @ ${prompt_tps} tok/s"
    fi
    _timer_end
}

# ─── Main ───────────────────────────────────────────────────────────────────

# Parse global flags
while [[ "${1:-}" == --* ]]; do
    case "$1" in
        --no-cache) NO_CACHE=true; shift ;;
        *) break ;;
    esac
done

CMD="${1:-}"
shift 2>/dev/null || true

case "$CMD" in
    ask)          cmd_ask "$@" ;;
    scan)         cmd_scan "$@" ;;
    classify)     cmd_classify "$@" ;;
    summarize)    cmd_summarize "$@" ;;
    triage)       cmd_triage "$@" ;;
    search-vuln)  cmd_search_vuln "$@" ;;
    bulk)         cmd_bulk "$@" ;;
    chunk)        cmd_chunk "$@" ;;
    pipe)         cmd_pipe "$@" ;;
    diff)         cmd_diff "$@" ;;
    cache-clean)  cmd_cache_clean ;;
    bench)        cmd_bench ;;
    stats)        cmd_stats ;;
    -v|--version) echo "gpu v${VERSION}" ;;
    -h|--help|help|"")
        echo -e "${C_BOLD}gpu v${VERSION}${C_RESET} — Bridge Híbrida Opus↔RTX4090"
        echo ""
        echo -e "${C_CYAN}Arquitetura:${C_RESET}  Opus decide → GPU extrai (180 tok/s) → Opus analisa"
        echo -e "${C_CYAN}Modelo:${C_RESET}       Qwen3-Coder-30B-MoE abliterated (178 tok/s, zero censura)"
        echo ""
        echo -e "${C_CYAN}Comandos:${C_RESET}"
        echo "  gpu ask \"pergunta\"                   Pergunta rápida"
        echo "  gpu scan <file> \"query\"              Varre arquivo"
        echo "  gpu classify <file>                  Classifica crash/log (JSON forçado)"
        echo "  gpu summarize <file>                 Resume arquivo grande"
        echo "  gpu triage <bugreport.zip|log>       Pipeline completo Android"
        echo "  gpu search-vuln <file> \"extras\"    Extrai padrões de risco (concretos, com validação)"
        echo "  gpu bulk [-r] [-p] <dir> \"instrução\" Processa pasta (-p = paralelo)"
        echo "  gpu chunk <file> \"instrução\" [lines] [overlap] Auto-fatia + paralelo + agrega"
        echo "  gpu pipe \"instrução\"                 Pipeline: stdin → GPU → stdout"
        echo "  gpu diff <f1> <f2> \"foco\"            Compara dois arquivos"
        echo "  gpu cache-clean                      Remove cache expirado (>24h)"
        echo "  gpu bench                            Benchmark rápido (mede tok/s real)"
        echo "  gpu stats                            Status do sistema"
        echo ""
        echo -e "${C_CYAN}Exemplos:${C_RESET}"
        echo "  gpu ask \"how to decode base64 in python\""
        echo "  gpu scan crash.log \"null pointer\""
        echo "  gpu summarize /var/log/syslog"
        echo "  gpu search-vuln app.py \"ssrf pickle\"  # padrões extras"
        echo "  gpu bulk -r -p src/ \"extract TODO comments\""
        echo "  gpu chunk bigfile.js \"extract bugs\" 800"
        echo "  gpu chunk bigfile.js \"extract bugs\" 800 100  # 800L chunks, 100L overlap"
        echo "  gpu scan file.js \"bugs\" | gpu pipe \"priorize por severidade\""
        echo "  gpu diff old.conf new.conf \"security changes\""
        echo ""
        echo ""
        echo -e "${C_CYAN}Flags globais:${C_RESET}"
        echo "  gpu --no-cache <cmd> ...     Ignora cache, força reprocessamento"
        echo ""
        echo -e "${C_CYAN}Exit codes:${C_RESET}"
        echo "  0  Sucesso"
        echo "  1  Erro genérico"
        echo "  2  Arquivo não encontrado / binário / sem permissão"
        echo "  3  Ollama offline / modelo indisponível"
        echo "  4  Timeout na query"
        echo "  5  Argumento faltando / uso incorreto"
        echo ""
        echo -e "${C_CYAN}Env:${C_RESET}"
        echo "  GPU_MODEL=modelo             Override modelo (default: qwen3-coder:30b)"
        echo "  OLLAMA_HOST=http://...       Host Ollama (default: localhost:11434)"
        echo "  GPU_TIMEOUT=180              Timeout em segundos"
        ;;
    *)
        log_err "Comando desconhecido: $CMD"
        echo "Use: gpu --help" >&2
        exit $EX_USAGE
        ;;
esac
